{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People Counter\n",
    "\n",
    "The people counter application is one of a series of IoT reference implementations aimed at instructing users on how to develop a working solution for a particular problem. It demonstrates how to create a smart video IoT solution using Intel® hardware and software tools. This solution detects people in a designated area, providing the number of people in the frame.\n",
    "\n",
    "## Overview of how it works\n",
    "At start-up the sample application reads the equivalent of command line arguments and loads a network and image from the video input to the Inference Engine (IE) plugin. A job is submitted to an edge compute node with a hardware accelerator such as Intel® HD Graphics GPU, Intel® Movidius™ Neural Compute Stick 2 and Intel® Arria® 10 FPGA.\n",
    "After the inference is completed, the output videos are appropriately stored in the /results/[device] directory, which can then be viewed within the Jupyter Notebook instance.\n",
    "\n",
    "## Demonstration objectives\n",
    "* Video as input is supported using **OpenCV**\n",
    "* Inference performed on edge hardware (rather than on the development node hosting this Jupyter notebook)\n",
    "* **OpenCV** provides the bounding boxes, labels and other information\n",
    "* Visualization of the resulting bounding boxes\n",
    "\n",
    "\n",
    "## Step 0: Set Up\n",
    "\n",
    "### 0.1: Import dependencies\n",
    "\n",
    "Run the below cell to import Python dependencies needed for displaying the results in this notebook\n",
    "(tip: select the cell and use **Ctrl+enter** to run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "from demoTools.demoutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2  (Optional-step): Original video without inference\n",
    "\n",
    "If you are curious to see the input video, run the following cell to view the original video stream used for inference and people counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -sf ./resources/Pedestrain_Detect_2_1_1.mp4\n",
    "videoHTML('People Counter Video', ['Pedestrain_Detect_2_1_1.mp4 '])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Using Intel® Distribution of OpenVINO™ toolkit\n",
    "\n",
    "We will be using Intel® Distribution of OpenVINO™ toolkit Inference Engine (IE) to locate people in frame.\n",
    "There are five steps involved in this task:\n",
    "\n",
    "1. Create an Intermediate Representation (IR) Model using the Model Optimizer by Intel\n",
    "2. Choose a device and create IEPlugin for the device\n",
    "3. Read the IRModel using IENetwork\n",
    "4. Load the IENetwork into the Plugin\n",
    "5. Run inference.\n",
    "\n",
    "### 1.1 Creating IR Model\n",
    "\n",
    "The Model Optimizer creates Intermediate Representation (IR) models that are optimized for different end-point target devices.\n",
    "These models can be created from existing DNN models from popular frameworks (e.g. Caffe*, TF) using the Model Optimizer. \n",
    "The Intel® Distribution of OpenVINO™ toolkit includes a utility script `model_downloader.py` that you can use to download some common models. Run the following cell to see the models available through `model_downloader.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The '!' is a special Jupyter Notebook command that allows you to run shell commands as if you are in a command line. So the above command will work straight out of the box on in a terminal (with '!' removed).\n",
    "\n",
    "Some of these downloaded models are already in the IR format, while others will require the model optimizer. In this demo, we will be using the **person-detection-retail-0013** model, which is already in IR format. This model can be downloaded with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name person-detection-retail-0013 -o models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input arguments are as follows:\n",
    "* --name : name of the model you want to download. It should be one of the models listed in the previous cell\n",
    "* -o : output directory. If this directory does not exist, it will be created for you.\n",
    "\n",
    "There are more arguments to this script and you can get the full list using the `-h` option.\n",
    "\n",
    "\n",
    "With the `-o` option set as above, this command downloads the model in the directory `models`, with the model files (.xml and .bin) located at `/Retail/object_detection/pedestrian/rmnet_ssd/0013/dldt`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Inference on a video\n",
    "\n",
    "The inference code is already implemented in \n",
    "<a href=\"main.py\">main.py</a>.\n",
    "\n",
    "The Python code takes in command line arguments for video, model etc.\n",
    "\n",
    "**Command line argument options and how they are interpreted in the application source code**\n",
    "\n",
    "```\n",
    "python3 main.py -m ${MODELPATH} \\\n",
    "                -i ${INPUT_FILE} \\\n",
    "                -o ${OUTPUT_FILE} \\\n",
    "                -d ${DEVICE} \\\n",
    "                -pt ${THRESHOLD}\\\n",
    "                -l /opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so\n",
    "\n",
    "```\n",
    "\n",
    "##### The description of the arguments used in the argument parser is the command line executable equivalent.\n",
    "* -m location of the pre-trained IR model which has been pre-processed using the model optimizer. There is automated support built in this argument to support both FP32 and FP16 models targeting different hardware\n",
    "* -i  location of the input video stream\n",
    "* -o location where the output file with inference needs to be stored (results/[device])\n",
    "* -d type of Hardware Acceleration (CPU, GPU, MYRIAD, HDDL or HETERO:FPGA,CPU)\n",
    "* -pt probability threshold value for the person detection\n",
    "* -l absolute path to the shared library and is currently optimized for core/xeon (/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating job file\n",
    "\n",
    "To run inference on the video, we need more compute power.\n",
    "We will run the workload on several edge compute nodes represented in the IoT DevCloud. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is written in Bash, and will be executed directly on the edge compute node.\n",
    "For this example, we have written the job file for you in the notebook.\n",
    "Run the following cell to write this in to the file \"people_counter.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile people_counter.sh\n",
    "\n",
    "#The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "#people_counter script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "#The output directory is the first argument of the bash script\n",
    "mkdir -p $1\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "FP_MODEL=$3\n",
    "INPUT_FILE=$4\n",
    "THRESHOLD=$5\n",
    "\n",
    "if [ $DEVICE = \"HETERO:FPGA,CPU\" ]; then\n",
    "    #Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    source /opt/fpga_support_files/setup_env.sh\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/2019R3_PV_PL1_FP11_YoloV3_ELU.aocx\n",
    "fi\n",
    "\n",
    "SAMPLEPATH=${PBS_O_WORKDIR}\n",
    "if [ \"$FP_MODEL\" = \"FP32\" ]; then\n",
    "  MODELPATH=${SAMPLEPATH}/models/Retail/object_detection/pedestrian/rmnet_ssd/0013/dldt/person-detection-retail-0013.xml\n",
    "else\n",
    "  MODELPATH=${SAMPLEPATH}/models/Retail/object_detection/pedestrian/rmnet_ssd/0013/dldt/person-detection-retail-0013-fp16.xml\n",
    "fi\n",
    "\n",
    "#Running the people counter code\n",
    "python3 people_counter.py   -m ${MODELPATH} \\\n",
    "                            -i ${INPUT_FILE} \\\n",
    "                            -o ${OUTPUT_FILE} \\\n",
    "                            -d ${DEVICE} \\\n",
    "                            -pt ${THRESHOLD}\\\n",
    "                            -l /opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Understand how jobs are submitted into the queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the `qsub` command.\n",
    "We can submit people_counter to several different types of edge compute nodes simultaneously or just one node at a time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- `-l` : this option let us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option let us send arguments to the bash script. \n",
    "- `-N` : this option let us name the job so that it is easier to distinguish between them.\n",
    "\n",
    "The `-F` flag is used to pass in arguments to the job script.\n",
    "The [people_counter.sh](people_counter.sh) takes in 5 arguments:\n",
    "1. the path to the directory for the output video and performance stats\n",
    "2. targeted device (e.g. CPU, GPU, MYRIAD, HDDL or HETERO:FPGA,CPU)\n",
    "3. the floating precision to use for inference\n",
    "4. location of the input video stream\n",
    "5. probability threshold value for the person detection\n",
    "\n",
    "The job scheduler will use the contents of `-F` flag as the argument to the job script.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     35 idc001skl,compnode,iei,tank-870,intel-core,i5-6500te,skylake,intel-hd-530,ram8gb,net1gbe\r\n",
      "     15 idc002mx8,compnode,iei,tank-870,intel-core,i5-6500te,skylake,intel-hd-530,ram8gb,net1gbe,hddl-r,iei-mustang-v100-mx8\r\n",
      "     17 idc003a10,compnode,iei,tank-870,intel-core,i5-6500te,skylake,intel-hd-530,ram8gb,net1gbe,hddl-f,iei-mustang-f100-a10\r\n",
      "     23 idc004nc2,compnode,iei,tank-870,intel-core,i5-6500te,skylake,intel-hd-530,ram8gb,net1gbe,ncs,intel-ncs2\r\n",
      "      6 idc006kbl,compnode,iei,tank-870,intel-core,i5-7500t,kaby-lake,intel-hd-630,ram8gb,net1gbe\r\n",
      "     13 idc007xv5,compnode,iei,tank-870,intel-xeon,e3-1268l-v5,skylake,intel-hd-p530,ram32gb,net1gbe\r\n",
      "     15 idc008u2g,compnode,up-squared,grove,intel-atom,e3950,apollo-lake,intel-hd-505,ram4gb,net1gbe,ncs,intel-ncs2\r\n",
      "      1 idc009jkl,compnode,jwip,intel-core,i5-7500,kaby-lake,intel-hd-630,ram8gb,net1gbe\r\n",
      "      1 idc010jal,compnode,jwip,intel-atom,e3950,apollo-lake,intel-hd-505,ram4gb,net1gbe\r\n",
      "      1 idc011ark2250s,compnode,advantech,intel-core,i5-6442eq,skylake,intel-hd-503,ram8gb,net1gbe\r\n",
      "      1 idc012ark1220l,compnode,advantech,intel-atom,e3940,apollo-lake,intel-hd-500,ram4gb,net1gbe\r\n",
      "      1 idc013ds580,compnode,advantech,intel-atom,e3950,apollo-lake,intel-hd-505,ram2gb,net1gbe\r\n",
      "      1 idc101agg,compnode,iei,tank-870,intel-core,i5-7500t,kaby-lake,intel-hd-630,ram8gb,net1gbe\r\n",
      "      1 idc101col,compnode,iei,tank-870,intel-core,i5-7500t,kaby-lake,intel-hd-630,ram8gb,net1gbe\r\n",
      "      1 idc101rdk,compnode,iei,tank-870,intel-core,i5-6500te,skylake,intel-hd-530,ram8gb,net1gbe,ncs,intel-ncs2\r\n",
      "      2 idc101ros,compnode,iei,tank-870,intel-core,i5-7500t,kaby-lake,intel-hd-630,ram8gb,net1gbe\r\n",
      "      3 idc101xv5,compnode,iei,tank-870,intel-xeon,e3-1268l-v5,skylake,intel-hd-p530,ram32gb,net1gbe\r\n"
     ]
    }
   ],
   "source": [
    "!pbsnodes | grep compnode | awk '{print $3}' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture.\n",
    "\n",
    "**Note**: If you want to use your own video, change the environment variable 'VIDEO' in the following cell from \"resources/Pedestrain_Detect_2_1_1.mp4\" to the full path of your uploaded video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VIDEO\"] = \"resources/Pedestrain_Detect_2_1_1.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Job queue submission\n",
    "\n",
    "Each of the cells below will submit a job to different edge compute nodes.\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all jobs at once or follow one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells)\n",
    "\n",
    "#### Submitting to an edge compute node with an Intel® CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® Core™ i5-6500TE processor</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub people_counter.sh -l nodes=1:tank-870:i5-6500te -F \"results/core/ CPU FP32 $VIDEO 0.7\" -N people_core\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/core/', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Xeon® CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel® \n",
    "    Xeon® Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5244.v-qsvr-1.devcloud-edge\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90c9aabbe98496ca68ebaef098ae4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, bar_style='info', description='Inference', style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub people_counter.sh  -l nodes=1:tank-870:e3-1268l-v5 -F \"results/xeon/ CPU FP32 $VIDEO 0.7\" -N people_xeon \n",
    "print(job_id_xeon[0]) \n",
    "#Progress indicators\n",
    "if job_id_xeon:\n",
    "    progressIndicator('results/xeon/', 'i_progress_'+job_id_xeon[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Core CPU and using the onboard Intel® GPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® Core i5-6500TE</a>. The inference workload will run on the Intel® HD Graphics 530 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5245.v-qsvr-1.devcloud-edge\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0211fd5c639a4a7bbdd1538f359646cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, bar_style='info', description='Inference', style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub people_counter.sh -l nodes=1:tank-870:i5-6500te:intel-hd-530 -F \"results/gpu/ GPU FP32 $VIDEO 0.7 \" -N people_gpu \n",
    "print(job_id_gpu[0]) \n",
    "#Progress indicators\n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/gpu/', 'i_progress_'+job_id_gpu[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® NCS 2 (Neural Compute Stick 2)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://software.intel.com/en-us/neural-compute-stick\">Intel Neural Compute Stick 2</a> installed in this  node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5246.v-qsvr-1.devcloud-edge\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3431aef50eb44d238b875b88c1730b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, bar_style='info', description='Inference', style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_id_ncs2 = !qsub people_counter.sh -l nodes=1:tank-870:i5-6500te:intel-ncs2 -F \"results/ncs2/ MYRIAD FP16 $VIDEO 0.7\" -N people_ncs2\n",
    "print(job_id_ncs2[0]) \n",
    "#Progress indicators\n",
    "if job_id_ncs2:\n",
    "    progressIndicator('results/ncs2/', 'i_progress_'+job_id_ncs2[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with UP Squared Grove IoT Development Kit (UP2)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/up-squared-grove-dev-kit\">UP Squared Grove IoT Development Kit</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/96488/Intel-Atom-x7-E3950-Processor-2M-Cache-up-to-2-00-GHz-\">Intel® Atom® x7-E3950 Processor</a>. The inference  workload will run on the integrated Intel® HD Graphics 505 card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5247.v-qsvr-1.devcloud-edge\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b201adcdc70940cd8641d209904bf093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, bar_style='info', description='Inference', style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_id_up2 = !qsub people_counter.sh -l nodes=1:up-squared  -F \"results/up2/ GPU FP32 $VIDEO 0.7\" -N people_up2\n",
    "print(job_id_up2[0]) \n",
    "#Progress indicators\n",
    "if job_id_up2:\n",
    "    progressIndicator('results/up2/', 'i_progress_'+job_id_up2[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core™ i5-6500te CPU</a> . The inference workload will run on the <a href=\"https://www.ieiworld.com/mustang-f100/en/\"> IEI Mustang-F100-A10 </a> card installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5248.v-qsvr-1.devcloud-edge\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c90fcd21ff4ebbbcce317ca3bd84fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, bar_style='info', description='Inference', style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_id_fpga = !qsub people_counter.sh -l nodes=1:tank-870:i5-6500te:iei-mustang-f100-a10 -F \"results/fpga/ HETERO:FPGA,CPU FP32 $VIDEO 0.7\" -N people_fpga\n",
    "print(job_id_fpga[0]) \n",
    "#Progress indicators\n",
    "if job_id_fpga:\n",
    "    progressIndicator('results/fpga/', 'i_progress_'+job_id_fpga[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use the `qstat` command.\n",
    "\n",
    "We have created a custom Jupyter widget  to get live qstat update.\n",
    "Run the following cell to bring it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d288ef2e28f4ce6ae8cf13854bc65b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid gray', width='100%'), outputs=({'output_type': 'stream', 'name': 'stdou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 2.3).\n",
    "There should also be an extra job in the queue \"jupyterhub\": this job runs your current Jupyter Notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "**Note**: Time spent in the queue depends on the number of users accessing the edge nodes. Once these jobs begin to run, they should take from 1 to 5 minutes to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Wait!***\n",
    "\n",
    "Please wait for the inference jobs and video rendering to complete before proceeding to the next step.\n",
    "\n",
    "## Step 3: View Results\n",
    "\n",
    "Once the jobs are completed, the queue system outputs the stdout and stderr streams of each job into files with names of the form\n",
    "\n",
    "`people_{type}.o{JobID}`\n",
    "\n",
    "`people_{type}.e{JobID}`\n",
    "\n",
    "(here, people_{type} corresponds to the `-N` option of qsub).\n",
    "\n",
    "However, for this case, we may be more interested in the output video files. They are stored in mp4 format inside the `results/[device]` directory.\n",
    "We wrote a short utility script that will display these videos within the notebook.\n",
    "Run the cells below to display them.\n",
    "See `demoutils.py` if you are interested in understanding further on how the results are displayed in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank (Intel Core CPU)', \n",
    "         ['results/core/people_counter.mp4'],'results/core/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Intel GPU (Intel Core + Onboard GPU)', \n",
    "          ['results/gpu/people_counter.mp4'],'results/gpu/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + Intel CPU + Intel NCS2', \n",
    "          ['results/ncs2/people_counter.mp4'],'results/ncs2/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank Xeon (Intel Xeon CPU)', \n",
    "          ['results/xeon/people_counter.mp4'],'results/xeon/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('UP Squared Grove IoT Development Kit (UP2)', \n",
    "          ['results/up2/people_counter.mp4'],'results/up2/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)', \n",
    "          ['results/fpga/people_counter.mp4'],'results/fpga/stats.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Assess Performance\n",
    "\n",
    "The running time of each inference task is recorded in `results/[device]/stats.txt`. Run the cell below to plot the results of all jobs side-by-side. Lower values mean better performance for **Inference Engine Processing Time** . Keep in mind that some architectures are optimized for the highest performance, others for low power or other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overload summaryPlot for Latency Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summaryPlot(results_list, x_axis, y_axis, title, plot):\n",
    "    ''' Bar plot input:\n",
    "\tresults_dict: dictionary of path to result file and label {path_to_result:label}\n",
    "\tx_axis: label of the x axis\n",
    "\ty_axis: label of the y axis\n",
    "\ttitle: title of the graph\n",
    "    '''\n",
    "    warnings.filterwarnings('ignore')\n",
    "    if plot=='time':\n",
    "        clr = 'xkcd:blue'\n",
    "    elif plot=='fps':\n",
    "        clr = 'xkcd:azure'\n",
    "    elif plot == \"latency\":\n",
    "        clr = 'xkcd:red'\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.title(title , fontsize=28, color='black', fontweight='bold')\n",
    "    plt.ylabel(y_axis, fontsize=16, color=clr)\n",
    "    plt.xlabel(x_axis, fontsize=16, color=clr)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    val = []\n",
    "    arch = []\n",
    "    diff = 0\n",
    "    for path, hw in results_list:\n",
    "        if os.path.isfile(path):\n",
    "            f = open(path, \"r\")\n",
    "            l1_time = float(f.readline())\n",
    "            l2_count = float(f.readline())\n",
    "            if plot==\"time\":\n",
    "                val.append(l1_time)\n",
    "            elif plot == \"fps\":\n",
    "                # print (l2_count/l1_time)\n",
    "                # print (\" for \" + str(hw) + \"\\n\")\n",
    "                # fps data\n",
    "                val.append((l2_count/l1_time))\n",
    "            elif plot == \"latency\":\n",
    "                batch = batch_dic[hw]\n",
    "                # print(str(hw) +  \"  \" + str(batch))\n",
    "                fps = float(batch) * 1.0/float(l2_count/l1_time) * 1000 # convert to ms\n",
    "                val.append(fps)\n",
    "            f.close()\n",
    "        else:\n",
    "            val.append(0)\n",
    "        arch.append(hw)\n",
    "\n",
    "    offset = max(val)/100\n",
    "    # print(\"offset: \" + str(offset)) text offset from the top of bar\n",
    "    for v in val:\n",
    "        if v == 0:\n",
    "            data = 'N/A'\n",
    "            y = 0\n",
    "        else:\n",
    "            precision = 2 \n",
    "            if v >= pow(10, precision):\n",
    "                data = '{:.0f}'.format(round(v/pow(10, precision+1), precision)*pow(10, precision+1))\n",
    "            else:\n",
    "                data = '{{:.{:d}g}}'.format(round(precision)).format(v)\n",
    "            y = v + offset\n",
    "            # print(\"y is: \" + str(y)) text vertical position\n",
    "        # print(\"data is: \" + str(data) + \" v is: \" + str(v))\n",
    "        plt.text(diff, y, data, fontsize=14, multialignment=\"center\",horizontalalignment=\"center\", verticalalignment=\"bottom\",  color='black')\n",
    "        diff += 1\n",
    "    # print(val)\n",
    "    plt.ylim(top=(max(val)+10*offset))\n",
    "    plt.bar(arch, val, width=0.8, align='center', color=clr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_list = [('core', 'Intel Core\\ni5-6500TE\\nCPU'),\n",
    "             ('xeon', 'Intel Xeon\\nE3-1268L v5\\nCPU'),\n",
    "             ('gpu', ' Intel Core\\ni5-6500TE\\nGPU'),\n",
    "             ('fpga', ' IEI Mustang\\nF100-A10\\nFPGA'),\n",
    "             ('hddlr', ' IEI Mustang\\nV100-MX8\\nVPU'),\n",
    "             ('ncs2', 'Intel\\nNCS2'),\n",
    "             ('up2', 'Intel Atom\\nx7-E3950\\nUP2/GPU')]\n",
    "\n",
    "# Batch size currently use for demo\n",
    "batch_dic = {\"Intel Core\\ni5-6500TE\\nCPU\": 1, \"Intel Xeon\\nE3-1268L v5\\nCPU\": 1,\n",
    "         \" Intel Core\\ni5-6500TE\\nGPU\": 1, \" IEI Mustang\\nF100-A10\\nFPGA\": 1,\n",
    "         \" IEI Mustang\\nV100-MX8\\nVPU\": 8, \"Intel\\nNCS2\": 1, \"Intel Atom\\nx7-E3950\\nUP2/GPU\": 1}\n",
    "\n",
    "stats_list = []\n",
    "for arch, a_name in arch_list:\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append(('results/stats_'+vars()['job_id_'+arch][0]+'.txt', a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "\n",
    "summaryPlot(stats_list, 'Architecture', 'Time, seconds', 'Inference Engine Processing Time', 'time' )\n",
    "summaryPlot(stats_list, 'Architecture', 'Frames per second', 'Inference Engine FPS', 'fps' )\n",
    "\n",
    "# Latency Plot\n",
    "summaryPlot(stats_list, 'Architecture', 'Time (ms)', 'Inference Engine Latency', 'latency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
